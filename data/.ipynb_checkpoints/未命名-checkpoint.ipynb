{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "import copy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"first_round_training_data.csv\")\n",
    "df2=pd.read_csv(\"first_round_testing_data.csv\")\n",
    "train_x=np.zeros([6000,10],dtype=np.float32)\n",
    "train_y=np.zeros([6000],dtype=np.int32)\n",
    "\n",
    "test_id=np.zeros([6000],dtype=np.int32)\n",
    "test_x=np.zeros([6000,10],dtype=np.float32)\n",
    "cls2int={\"Excellent\":0,\"Good\":1,\"Pass\":2,\"Fail\":3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    par=\"Parameter\"+str(i)\n",
    "    tmp=df1[par]\n",
    "    for j in range(6000):\n",
    "        train_x[j,i-1]=tmp[j]\n",
    "        cls=cls2int[df1[\"Quality_label\"][j]]\n",
    "        train_y[j]=cls\n",
    "for i in range(1,11):\n",
    "    par=\"Parameter\"+str(i)\n",
    "    tmp=df2[par]\n",
    "    for j in range(6000):\n",
    "        test_x[j,i-1]=tmp[j]\n",
    "        ID=int(df2[\"Group\"][j])\n",
    "        test_id[j]=ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain's multi_error: 0.363\n",
      "[200]\ttrain's multi_error: 0.296333\n",
      "[300]\ttrain's multi_error: 0.251667\n",
      "[400]\ttrain's multi_error: 0.208\n",
      "[500]\ttrain's multi_error: 0.1705\n",
      "[600]\ttrain's multi_error: 0.138333\n",
      "[700]\ttrain's multi_error: 0.109167\n",
      "[800]\ttrain's multi_error: 0.0861667\n",
      "[900]\ttrain's multi_error: 0.0676667\n",
      "[1000]\ttrain's multi_error: 0.0528333\n",
      "[1100]\ttrain's multi_error: 0.0395\n",
      "[1200]\ttrain's multi_error: 0.0305\n",
      "[1300]\ttrain's multi_error: 0.0236667\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "'boosting_type': 'gbdt',\n",
    "'objective': 'multiclassova',\n",
    "'num_class': 4,  \n",
    "'metric': 'multi_error', \n",
    "'num_leaves': 63,\n",
    "'learning_rate': 0.01,\n",
    "'feature_fraction': 0.9,\n",
    "'bagging_fraction': 0.9,\n",
    "'bagging_seed':0,\n",
    "'bagging_freq': 1,\n",
    "'verbose': -1,\n",
    "'reg_alpha':1,\n",
    "'reg_lambda':2,\n",
    "'lambda_l1': 0,\n",
    "'lambda_l2': 1,\n",
    "'num_threads': 8,\n",
    "}\n",
    "lgb_train = lgb.Dataset(train_x, train_y)\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=1300,\n",
    "                valid_sets=[lgb_train],\n",
    "                valid_names=['train'],\n",
    "                verbose_eval=100,\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"lgb1300round.csv\",\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=gbm.predict(test_x, num_iteration=1300)\n",
    "tmp=np.zeros([120,4])\n",
    "cnt=np.zeros([120])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Excellent ratio</th>\n",
       "      <th>Good ratio</th>\n",
       "      <th>Pass ratio</th>\n",
       "      <th>Fail ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.524425</td>\n",
       "      <td>0.276181</td>\n",
       "      <td>0.106221</td>\n",
       "      <td>0.012486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021734</td>\n",
       "      <td>0.182859</td>\n",
       "      <td>0.052346</td>\n",
       "      <td>0.708148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.030916</td>\n",
       "      <td>0.152574</td>\n",
       "      <td>0.079317</td>\n",
       "      <td>0.456325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.073676</td>\n",
       "      <td>0.398227</td>\n",
       "      <td>0.032273</td>\n",
       "      <td>0.370284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.026693</td>\n",
       "      <td>0.749609</td>\n",
       "      <td>0.160800</td>\n",
       "      <td>0.064106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Excellent ratio  Good ratio  Pass ratio  Fail ratio\n",
       "0         0.524425    0.276181    0.106221    0.012486\n",
       "1         0.021734    0.182859    0.052346    0.708148\n",
       "2         0.030916    0.152574    0.079317    0.456325\n",
       "3         0.073676    0.398227    0.032273    0.370284\n",
       "4         0.026693    0.749609    0.160800    0.064106"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pd.DataFrame(\n",
    "    ans, columns=[\"Excellent ratio\", \"Good ratio\", \"Pass ratio\", \"Fail ratio\"]\n",
    ")\n",
    "\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[\"Group\"] = df2[\"Group\"]\n",
    "group_pred = y_pred.groupby(\"Group\").mean()\n",
    "for c in [\"Excellent ratio\", \"Good ratio\", \"Pass ratio\", \"Fail ratio\"]:\n",
    "    group_pred[c] /= group_pred.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Excellent ratio</th>\n",
       "      <th>Good ratio</th>\n",
       "      <th>Pass ratio</th>\n",
       "      <th>Fail ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.207637</td>\n",
       "      <td>0.332453</td>\n",
       "      <td>0.252650</td>\n",
       "      <td>0.176934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.202872</td>\n",
       "      <td>0.223796</td>\n",
       "      <td>0.382088</td>\n",
       "      <td>0.165632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.222968</td>\n",
       "      <td>0.281879</td>\n",
       "      <td>0.324486</td>\n",
       "      <td>0.137495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.239824</td>\n",
       "      <td>0.267116</td>\n",
       "      <td>0.377346</td>\n",
       "      <td>0.077888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.155571</td>\n",
       "      <td>0.375789</td>\n",
       "      <td>0.231008</td>\n",
       "      <td>0.149235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.182149</td>\n",
       "      <td>0.233633</td>\n",
       "      <td>0.418529</td>\n",
       "      <td>0.149276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.209803</td>\n",
       "      <td>0.279578</td>\n",
       "      <td>0.311538</td>\n",
       "      <td>0.150953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.161243</td>\n",
       "      <td>0.218479</td>\n",
       "      <td>0.441710</td>\n",
       "      <td>0.139064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.105155</td>\n",
       "      <td>0.117050</td>\n",
       "      <td>0.685766</td>\n",
       "      <td>0.083339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.133196</td>\n",
       "      <td>0.250541</td>\n",
       "      <td>0.419053</td>\n",
       "      <td>0.167306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.080046</td>\n",
       "      <td>0.195664</td>\n",
       "      <td>0.464293</td>\n",
       "      <td>0.245467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.150765</td>\n",
       "      <td>0.236091</td>\n",
       "      <td>0.444786</td>\n",
       "      <td>0.134965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.262392</td>\n",
       "      <td>0.248943</td>\n",
       "      <td>0.440481</td>\n",
       "      <td>0.032378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.142517</td>\n",
       "      <td>0.254315</td>\n",
       "      <td>0.385278</td>\n",
       "      <td>0.179278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.334411</td>\n",
       "      <td>0.431318</td>\n",
       "      <td>0.182098</td>\n",
       "      <td>0.017641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.171915</td>\n",
       "      <td>0.243922</td>\n",
       "      <td>0.465320</td>\n",
       "      <td>0.095302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.243218</td>\n",
       "      <td>0.297367</td>\n",
       "      <td>0.413234</td>\n",
       "      <td>0.021219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.155686</td>\n",
       "      <td>0.205619</td>\n",
       "      <td>0.472114</td>\n",
       "      <td>0.127168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.142044</td>\n",
       "      <td>0.196093</td>\n",
       "      <td>0.611519</td>\n",
       "      <td>0.038991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.157351</td>\n",
       "      <td>0.253547</td>\n",
       "      <td>0.433197</td>\n",
       "      <td>0.118935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.166709</td>\n",
       "      <td>0.314312</td>\n",
       "      <td>0.348992</td>\n",
       "      <td>0.119801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.168325</td>\n",
       "      <td>0.252111</td>\n",
       "      <td>0.467322</td>\n",
       "      <td>0.083445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.180238</td>\n",
       "      <td>0.475054</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>0.069542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.145058</td>\n",
       "      <td>0.206469</td>\n",
       "      <td>0.484840</td>\n",
       "      <td>0.133162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.131050</td>\n",
       "      <td>0.485780</td>\n",
       "      <td>0.129196</td>\n",
       "      <td>0.213041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.193552</td>\n",
       "      <td>0.233523</td>\n",
       "      <td>0.432925</td>\n",
       "      <td>0.113909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.446988</td>\n",
       "      <td>0.182733</td>\n",
       "      <td>0.031671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.170201</td>\n",
       "      <td>0.232973</td>\n",
       "      <td>0.402354</td>\n",
       "      <td>0.151642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.334931</td>\n",
       "      <td>0.375420</td>\n",
       "      <td>0.198580</td>\n",
       "      <td>0.040454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.192364</td>\n",
       "      <td>0.221248</td>\n",
       "      <td>0.462912</td>\n",
       "      <td>0.092611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.244578</td>\n",
       "      <td>0.390599</td>\n",
       "      <td>0.176684</td>\n",
       "      <td>0.147425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.157861</td>\n",
       "      <td>0.242835</td>\n",
       "      <td>0.430658</td>\n",
       "      <td>0.132713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.197521</td>\n",
       "      <td>0.280761</td>\n",
       "      <td>0.233955</td>\n",
       "      <td>0.236567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.193211</td>\n",
       "      <td>0.257746</td>\n",
       "      <td>0.426580</td>\n",
       "      <td>0.091059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.216597</td>\n",
       "      <td>0.180620</td>\n",
       "      <td>0.413919</td>\n",
       "      <td>0.143842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.168119</td>\n",
       "      <td>0.251339</td>\n",
       "      <td>0.339326</td>\n",
       "      <td>0.199086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.187356</td>\n",
       "      <td>0.279732</td>\n",
       "      <td>0.257077</td>\n",
       "      <td>0.228268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.149147</td>\n",
       "      <td>0.212537</td>\n",
       "      <td>0.464052</td>\n",
       "      <td>0.146982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.160978</td>\n",
       "      <td>0.160590</td>\n",
       "      <td>0.576069</td>\n",
       "      <td>0.084991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.264304</td>\n",
       "      <td>0.452309</td>\n",
       "      <td>0.078161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.139706</td>\n",
       "      <td>0.175924</td>\n",
       "      <td>0.514274</td>\n",
       "      <td>0.174526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.206689</td>\n",
       "      <td>0.278612</td>\n",
       "      <td>0.390505</td>\n",
       "      <td>0.097915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.165344</td>\n",
       "      <td>0.303965</td>\n",
       "      <td>0.341966</td>\n",
       "      <td>0.130166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.155699</td>\n",
       "      <td>0.257088</td>\n",
       "      <td>0.404526</td>\n",
       "      <td>0.146054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.169603</td>\n",
       "      <td>0.344969</td>\n",
       "      <td>0.276994</td>\n",
       "      <td>0.194988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.140203</td>\n",
       "      <td>0.270891</td>\n",
       "      <td>0.429404</td>\n",
       "      <td>0.125031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.100272</td>\n",
       "      <td>0.090579</td>\n",
       "      <td>0.758334</td>\n",
       "      <td>0.044260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.180326</td>\n",
       "      <td>0.303219</td>\n",
       "      <td>0.382807</td>\n",
       "      <td>0.104941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.250542</td>\n",
       "      <td>0.269289</td>\n",
       "      <td>0.344094</td>\n",
       "      <td>0.089911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.199089</td>\n",
       "      <td>0.283144</td>\n",
       "      <td>0.379228</td>\n",
       "      <td>0.107269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.118985</td>\n",
       "      <td>0.297205</td>\n",
       "      <td>0.341436</td>\n",
       "      <td>0.214095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.192177</td>\n",
       "      <td>0.225406</td>\n",
       "      <td>0.433038</td>\n",
       "      <td>0.127899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.184746</td>\n",
       "      <td>0.301881</td>\n",
       "      <td>0.355012</td>\n",
       "      <td>0.123125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.171318</td>\n",
       "      <td>0.244819</td>\n",
       "      <td>0.443905</td>\n",
       "      <td>0.115882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.233106</td>\n",
       "      <td>0.349678</td>\n",
       "      <td>0.180983</td>\n",
       "      <td>0.182686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.149497</td>\n",
       "      <td>0.225981</td>\n",
       "      <td>0.456521</td>\n",
       "      <td>0.134728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.094342</td>\n",
       "      <td>0.084951</td>\n",
       "      <td>0.749354</td>\n",
       "      <td>0.059671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.196373</td>\n",
       "      <td>0.266398</td>\n",
       "      <td>0.348824</td>\n",
       "      <td>0.155847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.079420</td>\n",
       "      <td>0.077531</td>\n",
       "      <td>0.804117</td>\n",
       "      <td>0.031891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.192651</td>\n",
       "      <td>0.274496</td>\n",
       "      <td>0.384374</td>\n",
       "      <td>0.118673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Excellent ratio  Good ratio  Pass ratio  Fail ratio\n",
       "Group                                                     \n",
       "0             0.207637    0.332453    0.252650    0.176934\n",
       "1             0.202872    0.223796    0.382088    0.165632\n",
       "2             0.222968    0.281879    0.324486    0.137495\n",
       "3             0.239824    0.267116    0.377346    0.077888\n",
       "4             0.155571    0.375789    0.231008    0.149235\n",
       "5             0.182149    0.233633    0.418529    0.149276\n",
       "6             0.209803    0.279578    0.311538    0.150953\n",
       "7             0.161243    0.218479    0.441710    0.139064\n",
       "8             0.105155    0.117050    0.685766    0.083339\n",
       "9             0.133196    0.250541    0.419053    0.167306\n",
       "10            0.080046    0.195664    0.464293    0.245467\n",
       "11            0.150765    0.236091    0.444786    0.134965\n",
       "12            0.262392    0.248943    0.440481    0.032378\n",
       "13            0.142517    0.254315    0.385278    0.179278\n",
       "14            0.334411    0.431318    0.182098    0.017641\n",
       "15            0.171915    0.243922    0.465320    0.095302\n",
       "16            0.243218    0.297367    0.413234    0.021219\n",
       "17            0.155686    0.205619    0.472114    0.127168\n",
       "18            0.142044    0.196093    0.611519    0.038991\n",
       "19            0.157351    0.253547    0.433197    0.118935\n",
       "20            0.166709    0.314312    0.348992    0.119801\n",
       "21            0.168325    0.252111    0.467322    0.083445\n",
       "22            0.180238    0.475054    0.223000    0.069542\n",
       "23            0.145058    0.206469    0.484840    0.133162\n",
       "24            0.131050    0.485780    0.129196    0.213041\n",
       "25            0.193552    0.233523    0.432925    0.113909\n",
       "26            0.300464    0.446988    0.182733    0.031671\n",
       "27            0.170201    0.232973    0.402354    0.151642\n",
       "28            0.334931    0.375420    0.198580    0.040454\n",
       "29            0.192364    0.221248    0.462912    0.092611\n",
       "...                ...         ...         ...         ...\n",
       "90            0.244578    0.390599    0.176684    0.147425\n",
       "91            0.157861    0.242835    0.430658    0.132713\n",
       "92            0.197521    0.280761    0.233955    0.236567\n",
       "93            0.193211    0.257746    0.426580    0.091059\n",
       "94            0.216597    0.180620    0.413919    0.143842\n",
       "95            0.168119    0.251339    0.339326    0.199086\n",
       "96            0.187356    0.279732    0.257077    0.228268\n",
       "97            0.149147    0.212537    0.464052    0.146982\n",
       "98            0.160978    0.160590    0.576069    0.084991\n",
       "99            0.178735    0.264304    0.452309    0.078161\n",
       "100           0.139706    0.175924    0.514274    0.174526\n",
       "101           0.206689    0.278612    0.390505    0.097915\n",
       "102           0.165344    0.303965    0.341966    0.130166\n",
       "103           0.155699    0.257088    0.404526    0.146054\n",
       "104           0.169603    0.344969    0.276994    0.194988\n",
       "105           0.140203    0.270891    0.429404    0.125031\n",
       "106           0.100272    0.090579    0.758334    0.044260\n",
       "107           0.180326    0.303219    0.382807    0.104941\n",
       "108           0.250542    0.269289    0.344094    0.089911\n",
       "109           0.199089    0.283144    0.379228    0.107269\n",
       "110           0.118985    0.297205    0.341436    0.214095\n",
       "111           0.192177    0.225406    0.433038    0.127899\n",
       "112           0.184746    0.301881    0.355012    0.123125\n",
       "113           0.171318    0.244819    0.443905    0.115882\n",
       "114           0.233106    0.349678    0.180983    0.182686\n",
       "115           0.149497    0.225981    0.456521    0.134728\n",
       "116           0.094342    0.084951    0.749354    0.059671\n",
       "117           0.196373    0.266398    0.348824    0.155847\n",
       "118           0.079420    0.077531    0.804117    0.031891\n",
       "119           0.192651    0.274496    0.384374    0.118673\n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6000):\n",
    "    ID=test_id[i]\n",
    "    tmp[ID,:]+=ans[i,:]\n",
    "    cnt[ID]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50.,\n",
       "       50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50.,\n",
       "       50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50.,\n",
       "       50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50.,\n",
       "       50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50.,\n",
       "       50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50.,\n",
       "       50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50.,\n",
       "       50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50.,\n",
       "       50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50.,\n",
       "       50., 50., 50.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(120):\n",
    "    SUM=np.sum(tmp[i,:])\n",
    "    tmp[i,:]/=SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20763717, 0.33954862, 0.264883  , 0.18793121],\n",
       "       [0.20287201, 0.22775939, 0.39480497, 0.17456363],\n",
       "       [0.22296792, 0.28902054, 0.34078236, 0.14722918],\n",
       "       [0.23982419, 0.27570742, 0.39975806, 0.08471033],\n",
       "       [0.15557116, 0.39594033, 0.2686634 , 0.17982512],\n",
       "       [0.18214899, 0.23601207, 0.42726029, 0.15457865],\n",
       "       [0.20980321, 0.28949588, 0.33422754, 0.16647337],\n",
       "       [0.16124324, 0.2235293 , 0.46352397, 0.15170349],\n",
       "       [0.10515526, 0.11753855, 0.69148011, 0.08582608],\n",
       "       [0.1331956 , 0.25405062, 0.43449362, 0.17826016],\n",
       "       [0.08004641, 0.1964639 , 0.47045767, 0.25303201],\n",
       "       [0.15076451, 0.24036417, 0.46354057, 0.14533076],\n",
       "       [0.26239229, 0.25257681, 0.45141022, 0.03362068],\n",
       "       [0.14251699, 0.2592431 , 0.4041672 , 0.1940727 ],\n",
       "       [0.33441097, 0.44979355, 0.19659443, 0.01920106],\n",
       "       [0.17191467, 0.24745639, 0.47998329, 0.10064565],\n",
       "       [0.24321755, 0.3038053 , 0.4304589 , 0.02251826],\n",
       "       [0.15568625, 0.21029881, 0.49483913, 0.13917582],\n",
       "       [0.14204428, 0.19737732, 0.62027057, 0.04030783],\n",
       "       [0.15735089, 0.25886916, 0.45463588, 0.12914407],\n",
       "       [0.16670852, 0.32379783, 0.37607192, 0.13342173],\n",
       "       [0.16832504, 0.25655654, 0.48582784, 0.08929058],\n",
       "       [0.18023769, 0.49230326, 0.24859191, 0.07886713],\n",
       "       [0.14505819, 0.2098178 , 0.50227559, 0.14284843],\n",
       "       [0.13104999, 0.49560345, 0.14022353, 0.23312304],\n",
       "       [0.19355233, 0.23766782, 0.4480863 , 0.12069356],\n",
       "       [0.30046426, 0.46610292, 0.19867944, 0.03475338],\n",
       "       [0.17020075, 0.23904111, 0.4247374 , 0.16602074],\n",
       "       [0.33493109, 0.3990951 , 0.22043816, 0.04553566],\n",
       "       [0.19236441, 0.22601313, 0.48214456, 0.0994779 ],\n",
       "       [0.16804878, 0.195242  , 0.53410733, 0.10260189],\n",
       "       [0.15509158, 0.29810398, 0.3824752 , 0.16432923],\n",
       "       [0.06452511, 0.06207781, 0.82799687, 0.04540021],\n",
       "       [0.15240804, 0.27090929, 0.44201375, 0.13466892],\n",
       "       [0.0922558 , 0.07434788, 0.78560651, 0.04778981],\n",
       "       [0.20699216, 0.2544117 , 0.41288803, 0.1257081 ],\n",
       "       [0.17562949, 0.22276408, 0.50130271, 0.10030372],\n",
       "       [0.1544709 , 0.22837525, 0.50112716, 0.11602668],\n",
       "       [0.15806204, 0.13860184, 0.625083  , 0.07825311],\n",
       "       [0.17804783, 0.28376463, 0.37633447, 0.16185308],\n",
       "       [0.42301398, 0.25254271, 0.29702754, 0.02741577],\n",
       "       [0.18325368, 0.25650572, 0.42407663, 0.13616397],\n",
       "       [0.16864752, 0.21130131, 0.53972193, 0.08032924],\n",
       "       [0.14681286, 0.23960614, 0.48993863, 0.12364237],\n",
       "       [0.12724808, 0.20544322, 0.445865  , 0.2214437 ],\n",
       "       [0.14138377, 0.20376465, 0.52871885, 0.12613272],\n",
       "       [0.11984777, 0.10152277, 0.74433485, 0.03429461],\n",
       "       [0.15891054, 0.26387122, 0.40313597, 0.17408227],\n",
       "       [0.16509892, 0.32143977, 0.40145515, 0.11200617],\n",
       "       [0.14474551, 0.26477355, 0.39026199, 0.20021895],\n",
       "       [0.07515724, 0.11147258, 0.75804808, 0.05532209],\n",
       "       [0.17897117, 0.28687152, 0.36653016, 0.16762715],\n",
       "       [0.09938298, 0.08283866, 0.78392507, 0.0338533 ],\n",
       "       [0.11787   , 0.28525362, 0.38546784, 0.21140854],\n",
       "       [0.14781172, 0.12474514, 0.70324848, 0.02419465],\n",
       "       [0.15313922, 0.27271484, 0.38278716, 0.19135879],\n",
       "       [0.07805059, 0.08072495, 0.79925835, 0.04196612],\n",
       "       [0.21066582, 0.20435375, 0.45638626, 0.12859418],\n",
       "       [0.06485959, 0.07483941, 0.8191497 , 0.04115129],\n",
       "       [0.16864739, 0.27902027, 0.41342946, 0.13890288],\n",
       "       [0.10527205, 0.08780865, 0.78236005, 0.02455925],\n",
       "       [0.18083965, 0.23389272, 0.48412328, 0.10114435],\n",
       "       [0.1129922 , 0.2310048 , 0.39861404, 0.25738896],\n",
       "       [0.12957818, 0.23336901, 0.54726279, 0.08979002],\n",
       "       [0.14827354, 0.23044312, 0.41628493, 0.20499841],\n",
       "       [0.16011182, 0.23286468, 0.41565073, 0.19137277],\n",
       "       [0.1687238 , 0.25657187, 0.39308066, 0.18162367],\n",
       "       [0.17791256, 0.27230298, 0.41225496, 0.1375295 ],\n",
       "       [0.22970127, 0.26346298, 0.3585657 , 0.14827005],\n",
       "       [0.23327353, 0.2435329 , 0.39706951, 0.12612405],\n",
       "       [0.11747315, 0.40977761, 0.17858429, 0.29416494],\n",
       "       [0.18863418, 0.26935623, 0.40335272, 0.13865686],\n",
       "       [0.21373451, 0.30359519, 0.2967462 , 0.18592411],\n",
       "       [0.16747584, 0.22913872, 0.45863505, 0.14475039],\n",
       "       [0.1053043 , 0.19775691, 0.48867508, 0.20826371],\n",
       "       [0.1794959 , 0.25749826, 0.44206606, 0.12093978],\n",
       "       [0.08569457, 0.3765128 , 0.25270687, 0.28508576],\n",
       "       [0.16188737, 0.22813025, 0.44881896, 0.16116341],\n",
       "       [0.10063599, 0.19481758, 0.59570878, 0.10883766],\n",
       "       [0.17669051, 0.25345906, 0.41452544, 0.155325  ],\n",
       "       [0.08722316, 0.28624225, 0.41291433, 0.21362026],\n",
       "       [0.12376021, 0.23213414, 0.49230317, 0.15180247],\n",
       "       [0.11335282, 0.20231181, 0.37200452, 0.31233085],\n",
       "       [0.16693618, 0.26704983, 0.39145396, 0.17456003],\n",
       "       [0.12483836, 0.20857259, 0.48285764, 0.18373141],\n",
       "       [0.20925002, 0.25757652, 0.43391476, 0.0992587 ],\n",
       "       [0.24917291, 0.3153956 , 0.3258404 , 0.10959109],\n",
       "       [0.17621617, 0.2394299 , 0.44839896, 0.13595497],\n",
       "       [0.24160455, 0.35947548, 0.25639267, 0.1425273 ],\n",
       "       [0.181017  , 0.27070348, 0.44898787, 0.09929165],\n",
       "       [0.24457793, 0.4043708 , 0.19042889, 0.16062238],\n",
       "       [0.15786058, 0.2477737 , 0.45076256, 0.14360315],\n",
       "       [0.19752062, 0.29067555, 0.2516416 , 0.26016223],\n",
       "       [0.19321056, 0.26331716, 0.4457245 , 0.09774777],\n",
       "       [0.21659697, 0.18697664, 0.43800423, 0.15842215],\n",
       "       [0.16811877, 0.25753263, 0.35808769, 0.21626091],\n",
       "       [0.18735561, 0.28838742, 0.27468356, 0.24957342],\n",
       "       [0.14914722, 0.21563651, 0.4790239 , 0.15619237],\n",
       "       [0.16097838, 0.16234526, 0.58763689, 0.08903948],\n",
       "       [0.17873523, 0.26879636, 0.46933392, 0.08313449],\n",
       "       [0.1397059 , 0.17554701, 0.51197707, 0.17277001],\n",
       "       [0.20668906, 0.28385977, 0.40572536, 0.10372582],\n",
       "       [0.16534382, 0.3147074 , 0.37259605, 0.14735272],\n",
       "       [0.15569936, 0.26228177, 0.42409195, 0.15792693],\n",
       "       [0.16960283, 0.34756381, 0.2825973 , 0.20023606],\n",
       "       [0.14020313, 0.27559138, 0.44923412, 0.13497137],\n",
       "       [0.10027202, 0.09090554, 0.7632866 , 0.04553585],\n",
       "       [0.18032617, 0.30869227, 0.39923904, 0.11174252],\n",
       "       [0.25054218, 0.28038356, 0.3696894 , 0.09938486],\n",
       "       [0.19908905, 0.28928756, 0.39682993, 0.11479347],\n",
       "       [0.11898501, 0.3006273 , 0.35402079, 0.2263669 ],\n",
       "       [0.19217691, 0.22863753, 0.44512989, 0.13405567],\n",
       "       [0.18474645, 0.30873631, 0.37381341, 0.13270382],\n",
       "       [0.17131847, 0.24837921, 0.45800666, 0.12229565],\n",
       "       [0.23310633, 0.36524951, 0.19831397, 0.20333019],\n",
       "       [0.14949707, 0.23004917, 0.47530583, 0.14514793],\n",
       "       [0.09434156, 0.08546117, 0.75751899, 0.06267828],\n",
       "       [0.19637341, 0.27224071, 0.36481502, 0.16657086],\n",
       "       [0.07942023, 0.0778122 , 0.8096614 , 0.03310616],\n",
       "       [0.19265116, 0.27996873, 0.40085405, 0.12652606]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain's multi_error: 0.363\n",
      "[200]\ttrain's multi_error: 0.296333\n",
      "[300]\ttrain's multi_error: 0.251667\n",
      "[400]\ttrain's multi_error: 0.208\n",
      "[500]\ttrain's multi_error: 0.1705\n",
      "[600]\ttrain's multi_error: 0.138333\n",
      "[700]\ttrain's multi_error: 0.109167\n",
      "[800]\ttrain's multi_error: 0.0861667\n",
      "[900]\ttrain's multi_error: 0.0676667\n",
      "[1000]\ttrain's multi_error: 0.0528333\n",
      "[1100]\ttrain's multi_error: 0.0395\n",
      "[1200]\ttrain's multi_error: 0.0305\n",
      "[1300]\ttrain's multi_error: 0.0236667\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'submit/lgb1300round.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-0fc507c11c53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 )\n\u001b[1;32m--> 107\u001b[1;33m \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"submit/lgb1300round.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[0mans\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'submit/lgb1300round.csv'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "import copy as cp\n",
    "df1=pd.read_csv(\"first_round_training_data.csv\")\n",
    "df2=pd.read_csv(\"first_round_testing_data.csv\")\n",
    "train_x=np.zeros([6000,10],dtype=np.float32)\n",
    "train_y=np.zeros([6000],dtype=np.int32)\n",
    "\n",
    "test_id=np.zeros([6000],dtype=np.int32)\n",
    "test_x=np.zeros([6000,10],dtype=np.float32)\n",
    "cls2int={\"Excellent\":0,\"Good\":1,\"Pass\":2,\"Fail\":3}\n",
    "for i in range(1,11):\n",
    "    par=\"Parameter\"+str(i)\n",
    "    tmp=df1[par]\n",
    "    for j in range(6000):\n",
    "        train_x[j,i-1]=tmp[j]\n",
    "        cls=cls2int[df1[\"Quality_label\"][j]]\n",
    "        train_y[j]=cls\n",
    "for i in range(1,11):\n",
    "    par=\"Parameter\"+str(i)\n",
    "    tmp=df2[par]\n",
    "    for j in range(6000):\n",
    "        test_x[j,i-1]=tmp[j]\n",
    "        ID=int(df2[\"Group\"][j])\n",
    "        test_id[j]=ID\n",
    "d_train = pd.read_csv(\"../data/first_round_training_data.csv\")\n",
    "d_test = pd.read_csv(\"../data/first_round_testing_data.csv\")\n",
    "\n",
    "########################################################################################################################\n",
    "# ORI: 处理类别标签并分割x,y\n",
    "#\n",
    "# for i in range(1, 11):\n",
    "#     par = \"Parameter\" + str(i)\n",
    "#     tmp = d_train[par]\n",
    "#     for j in range(6000):\n",
    "#         x_train[j, i - 1] = tmp[j]\n",
    "#         cls = cls2int[d_train[\"Quality_label\"][j]]\n",
    "#         y_train[j] = cls\n",
    "# lgb_train = lgb.Dataset(x_train, y_train)\n",
    "\n",
    "lgb_train = lgb.Dataset(\n",
    "    d_train[[\"Parameter\" + str(i) for i in range(1, 11)]],\n",
    "    d_train[\"Quality_label\"].map({\"Excellent\": 0, \"Good\": 1, \"Pass\": 2, \"Fail\": 3}),\n",
    ")\n",
    "########################################################################################################################\n",
    "\n",
    "########################################################################################################################\n",
    "# ORI: 丢弃Group信息\n",
    "#\n",
    "# for i in range(1, 11):\n",
    "#     par = \"Parameter\" + str(i)\n",
    "#     tmp = d_test[par]\n",
    "#     for j in range(6000):\n",
    "#         x_test[j, i - 1] = tmp[j]\n",
    "#         ID = int(d_test[\"Group\"][j])\n",
    "#         test_id[j] = ID\n",
    "x_test = d_test.drop([\"Group\"], axis=1)\n",
    "########################################################################################################################\n",
    "\n",
    "# lgbm params\n",
    "params = {\n",
    "'boosting_type': 'gbdt',\n",
    "'objective': 'multiclassova',\n",
    "'num_class': 4,\n",
    "'metric': 'multi_error',\n",
    "'num_leaves': 63,\n",
    "'learning_rate': 0.01,\n",
    "'feature_fraction': 0.9,\n",
    "'bagging_fraction': 0.9,\n",
    "'bagging_seed':0,\n",
    "'bagging_freq': 1,\n",
    "'verbose': -1,\n",
    "'reg_alpha':1,\n",
    "'reg_lambda':2,\n",
    "'lambda_l1': 0,\n",
    "'lambda_l2': 1,\n",
    "'num_threads': 8,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"multiclassova\",\n",
    "    \"num_class\": 4,\n",
    "    \"metric\": \"multi_error\",\n",
    "    \"num_leaves\": 63,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.9,\n",
    "    \"bagging_seed\": 0,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"verbose\": -1,\n",
    "    \"reg_alpha\": 1,\n",
    "    \"reg_lambda\": 2,\n",
    "    \"lambda_l1\": 0,\n",
    "    \"lambda_l2\": 1,\n",
    "    \"num_threads\": 8,\n",
    "}\n",
    "lgb_train = lgb.Dataset(train_x, train_y)\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=1300,\n",
    "                valid_sets=[lgb_train],\n",
    "                valid_names=['train'],\n",
    "                verbose_eval=100,\n",
    "                )\n",
    "f=open(\"lgb1300round.csv\",\"w\")\n",
    "ans=gbm.predict(test_x, num_iteration=1300)\n",
    "tmp=np.zeros([120,4])\n",
    "cnt=np.zeros([120])\n",
    "for i in range(6000):\n",
    "    ID=test_id[i]\n",
    "    tmp[ID,:]+=ans[i,:]\n",
    "    cnt[ID]+=1\n",
    "for i in range(120):\n",
    "    SUM=np.sum(tmp[i,:])\n",
    "    tmp[i,:]/=SUM\n",
    "\n",
    "f.write(\"Group,Excellent ratio,Good ratio,Pass ratio,Fail ratio\\n\")\n",
    "for i in range(120):\n",
    "    f.write(str(i))\n",
    "    for j in range(4):\n",
    "        f.write(\",\"+str(tmp[i,j]))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "# train\n",
    "gbm = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round=1300,\n",
    "    valid_sets=[lgb_train],\n",
    "    valid_names=[\"train\"],\n",
    "    verbose_eval=100,\n",
    ")\n",
    "\n",
    "# predict\n",
    "y_pred = gbm.predict(x_test, num_iteration=1300)\n",
    "\n",
    "########################################################################################################################\n",
    "# ORI: 将结果丢进分组里,并整体缩放,保证概率和为1\n",
    "#\n",
    "# f = open(\"./lgb1300round.csv\", \"w\")\n",
    "# tmp = np.zeros([120, 4])\n",
    "# cnt = np.zeros([120])\n",
    "# for i in range(6000):\n",
    "#     ID = test_id[i]\n",
    "#     tmp[ID, :] += ans[i, :]\n",
    "#     cnt[ID] += 1\n",
    "# for i in range(120):\n",
    "#     SUM = np.sum(tmp[i, :])\n",
    "#     tmp[i, :] /= SUM\n",
    "#\n",
    "# f.write(\"Group,Excellent ratio,Good ratio,Pass ratio,Fail ratio\\n\")\n",
    "# for i in range(120):\n",
    "#     f.write(str(i))\n",
    "#     for j in range(4):\n",
    "#         f.write(\",\" + str(tmp[i, j]))\n",
    "#     f.write(\"\\n\")\n",
    "# f.close()\n",
    "y_pred = pd.DataFrame(\n",
    "    y_pred, columns=[\"Excellent ratio\", \"Good ratio\", \"Pass ratio\", \"Fail ratio\"]\n",
    ")\n",
    "\n",
    "y_pred[\"Group\"] = d_test[\"Group\"]\n",
    "group_pred = y_pred.groupby(\"Group\").mean()\n",
    "for c in [\"Excellent ratio\", \"Good ratio\", \"Pass ratio\", \"Fail ratio\"]:\n",
    "    group_pred[c] /= group_pred.sum(axis=1)\n",
    "########################################################################################################################\n",
    "group_pred.to_csv(\"lgb_submit.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
