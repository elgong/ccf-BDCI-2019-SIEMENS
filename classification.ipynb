{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入包\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.py\n",
      "first_round_testing_data.csv\n",
      "first_round_training_data.csv\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "!ls data/\n",
    "\n",
    "df_train = pd.read_csv(\"data\\\\first_round_training_data.csv\")\n",
    "\n",
    "# 去掉多余的特征\n",
    "#df_train = df_train.drop(df_train.columns[[10,11,12,13,14,15,16,17,18,19,]], axis=1, )\n",
    "df_train = df_train.drop(df_train.columns[[10,11,12,13,14,15 , 16,17,18,19]], axis=1, )\n",
    "\n",
    "\n",
    "# 特征10 换位到最后一列\n",
    "df_test = pd.read_csv(\"data\\\\first_round_testing_data.csv\")\n",
    "tmp = df_test[\"Parameter10\"].copy()\n",
    "df_test.drop(\"Parameter10\",axis=1, inplace=True)\n",
    "df_test[\"Parameter10\"] = tmp\n",
    "\n",
    "# 制作标签\n",
    "def makeLabel(row):\n",
    "    #print(row)\n",
    "    if row[\"Quality_label\"] == \"Fail\":\n",
    "        return 0\n",
    "    elif row[\"Quality_label\"] == \"Pass\":\n",
    "        return 1\n",
    "    elif row[\"Quality_label\"] == \"Good\":\n",
    "        return 2\n",
    "    elif row[\"Quality_label\"] == \"Excellent\":\n",
    "        return 3\n",
    "\n",
    "df_train[\"Quality_label\"] = df_train[\"Quality_label\"].map({\"Excellent\":3,\"Good\":2,\"Pass\":1,\"Fail\":0}) \n",
    "df_train.head()\n",
    "\n",
    "\n",
    "\n",
    "# 划分数据集\n",
    "df_target = df_train[\"Quality_label\"]\n",
    "train = df_train.drop([\"Quality_label\"], axis=1)\n",
    "\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(train,df_target,test_size=0.3, random_state=0)\n",
    "### model-4 xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "score_train = []\n",
    "score_test = []\n",
    "c_list = [i for i in range(1, 10,1)]\n",
    "#c_list = []\n",
    "#for c in c_list:\n",
    "    \n",
    "reg = xgb.XGBClassifier(\n",
    "      max_depth=5    # step2\n",
    "    , learning_rate=0.1\n",
    "    , n_estimators=29  # step1\n",
    "    , verbosity=1\n",
    "    , silent=None\n",
    "    , objective=\"binary:logistic\"\n",
    "    , booster='gbtree'\n",
    "    , n_jobs=1\n",
    "    , nthread=None\n",
    "    , gamma=0\n",
    "    , min_child_weight=5   # step3\n",
    "\n",
    "    , max_delta_step=0\n",
    "    , subsample=1\n",
    "    , colsample_bytree=0.7\n",
    "    , colsample_bylevel=1\n",
    "    , colsample_bynode=1\n",
    "    , reg_alpha=0.3\n",
    "    , reg_lambda=1\n",
    "    , scale_pos_weight=1\n",
    "    , base_score=0.5\n",
    "    , random_state=2019\n",
    "    , seed=None\n",
    "    , missing=None\n",
    "    , importance_type=\"gain\"\n",
    "    )\n",
    "reg = reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# train_score = reg.score(X_train, y_train)\n",
    "# score_train.append(train_score)\n",
    "\n",
    "# test_score = reg.score(X_test, y_test)\n",
    "# score_test.append(test_score)\n",
    "    \n",
    "# plt.figure(figsize=[8,4])\n",
    "# plt.plot(c_list,score_test)\n",
    "# print(max(score_train))\n",
    "# print(max(score_test))\n",
    "# print(c_list[list(score_test).index(max(list(score_test)))])\n",
    "reg.get_booster().save_model('xgb.model')\n",
    "# model = xgb.XGBClassifier(max_depth=5, n_estimators=200, learn_rate=0.01)\n",
    "# model.fit(X_train, y_train)  \n",
    "# test_score = model.score(X_test, y_test)\n",
    "# print('test_score: {0}'.format(test_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(res, target):\n",
    "    if len(res) != len(target):\n",
    "        raise Exception(\"数据错误\")\n",
    "    \n",
    "    trueNum = 0\n",
    "    for i in range(len(res)):\n",
    "        if res[i] == y_train.iloc[i]:\n",
    "            trueNum += 1\n",
    "    print(float(trueNum/len(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_score: 0.49944444444444447\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集\n",
    "df_target = df_train[\"Quality_label\"]\n",
    "train = df_train.drop([\"Quality_label\"], axis=1)\n",
    "\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(train,df_target,test_size=0.3, random_state=0)\n",
    "### model-4 xgboost\n",
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test,label=y_test)\n",
    "\n",
    "\n",
    "model = xgb.XGBClassifier(max_depth=5, n_estimators=200, learn_rate=0.01)\n",
    "model.fit(X_train, y_train)  \n",
    "test_score = model.score(X_test, y_test)\n",
    "print('test_score: {0}'.format(test_score))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#对数据的训练集进行标准化\n",
    "\n",
    "# x = X_train.copy()\n",
    "# ss = StandardScaler()\n",
    "# X= ss.fit_transform(x)\n",
    "\n",
    "\n",
    "## model -1 逻辑回归\n",
    "# clf = LogisticRegression(random_state=2, solver='lbfgs',multi_class='multinomial',max_iter = 1000).fit(X, y_train)\n",
    "# print(clf.score(X,y_train))\n",
    "\n",
    "\n",
    "### model -2  决策树 \n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# treeClf = DecisionTreeClassifier(\n",
    "#                                      max_depth=7\n",
    "#                                    # ,splitter = \"random\"\n",
    "#                                     ,min_samples_split = 40\n",
    "#                                     ,random_state = 6\n",
    "    \n",
    "#                                         )\n",
    "# treeClf = treeClf.fit(X_train, y_train)\n",
    "# res = treeClf.score(X_test,y_test)\n",
    "# print(res)\n",
    "\n",
    "### model-3  随机森林  # 最好成绩 0.44\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# rfc = RandomForestClassifier(\n",
    "#                           n_estimators=100  # 默认10，决策树的数量, 越多越好，从100开始。\n",
    "                          \n",
    "#                         , bootstrap=True   # 有放回采样\n",
    "#                         , oob_score=False  # out-of-bag 使用没有抽到的数据做验证\n",
    "#                         , n_jobs=None      #并行计算\n",
    "#                         , verbose=0        # 打印训练日志\n",
    "#                         , warm_start=False  # 热启动\n",
    "                        \n",
    "#                         #与决策树相关的参数\n",
    "#                         , criterion='gini'  # 或 \"entropy\" \n",
    "#                         , random_state=None\n",
    "#                         , class_weight=None\n",
    "#                         , max_depth=None\n",
    "#                         , min_samples_split=2\n",
    "#                         , min_samples_leaf=1\n",
    "#                         , min_weight_fraction_leaf=0.0\n",
    "#                         , max_features='auto' # auto=特征数的开根号，log2=log2(n_features)， None = n_features\n",
    "#                         , max_leaf_nodes=None\n",
    "#                         , min_impurity_decrease=0.0\n",
    "#                         , min_impurity_split=None\n",
    "#                         )\n",
    "\n",
    "# rfc = rfc.fit(X_train, y_train)\n",
    "# score = rfc.score(X_test,y_test)\n",
    "# print(score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每轮迭代运行结果:[0.49583333 0.4985     0.5005     0.50266667 0.49383333 0.498\n",
      " 0.49816667 0.49883333]\n",
      "参数的最佳取值：{'random_state': 2019}\n",
      "最佳模型得分:0.5026666666666667\n"
     ]
    }
   ],
   "source": [
    "### 调参过程\n",
    "\n",
    "# score_list = []\n",
    "# for c in range(340,380,2):\n",
    "#     model = xgb.XGBClassifier(\n",
    "#           max_depth=5\n",
    "#         , learning_rate=0.1\n",
    "#         , n_estimators=c   # step1: 360\n",
    "#         , verbosity=1\n",
    "#         , silent=None\n",
    "#         , objective='reg:squarederror'\n",
    "#         , booster='gbtree'\n",
    "#         , min_child_weight=1\n",
    "\n",
    "#         , scale_pos_weight=1\n",
    "#         , base_score=0.5\n",
    "#         , random_state=0\n",
    "#         , seed=None\n",
    "#         , missing=None\n",
    "#         , importance_type='gain',\n",
    "        \n",
    "#         )\n",
    "#     model.fit(X_train, y_train)  \n",
    "#     test_score = model.score(X_test, y_test)\n",
    "#     #print('test_score: {0}'.format(test_score))\n",
    "#     score_list.append(test_score)\n",
    "# print(\"best n_estimators: \", range(340,380,2)[score_list.index(max(score_list))])\n",
    "# print(\"best score:\", max(score_list))\n",
    "# plt.figure(figsize=[8,4])\n",
    "# plt.plot(range(340,380,2),score_list)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "# dtest = xgb.DMatrix(X_test,label=y_test)\n",
    "cv_params = {'random_state': [42,2,3,2019,2000,10,1,6666]}\n",
    "other_params = {    \n",
    "                   # step1: 24\n",
    "                    \"n_estimators\":200\n",
    "    \n",
    "                  # step2: 4, 3\n",
    "                    , \"max_depth\":4\n",
    "                    , \"min_child_weight\" : 3\n",
    "    \n",
    "                  # step3:\n",
    "                    , \"subsample\" : 0.7\n",
    "                    , \"colsample_bytree\" : 1\n",
    "    \n",
    "                \n",
    "                    , \"learning_rate\":0.1\n",
    "                \n",
    "                    , \"verbosity\":1\n",
    "                    , \"silent\":None\n",
    "                    , \"objective\":'binary:logistic'\n",
    "                    , \"booster\": \"gbtree\"\n",
    "                    , \"n_jobs\" : 1\n",
    "                    , \"nthread\" : None\n",
    "                    , \"gamma\" : 0\n",
    "                  \n",
    "                    , \"max_delta_step\" : 0\n",
    "\n",
    "                    , \"colsample_bylevel\" : 1\n",
    "                    , \"colsample_bynode\" : 1\n",
    "                    , \"reg_alpha\" : 0\n",
    "                    , \"reg_lambda\" : 1\n",
    "                    , \"scale_pos_weight\" : 1\n",
    "                    , \"base_score\" : 0.5\n",
    "                  #  , \"random_state\" : 42\n",
    "                    , \"seed\"  : None\n",
    "                    , \"missing\" : None\n",
    "                    , \"importance_type\" :'gain'\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(**other_params)\n",
    "optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring='accuracy', cv=5, verbose=1, n_jobs=1)\n",
    "optimized_GBM.fit(train, df_target)\n",
    "\n",
    "evalute_result = optimized_GBM.cv_results_['mean_test_score']\n",
    "print('每轮迭代运行结果:{0}'.format(evalute_result))\n",
    "print('参数的最佳取值：{0}'.format(optimized_GBM.best_params_))\n",
    "print('最佳模型得分:{0}'.format(optimized_GBM.best_score_))\n",
    "\n",
    "# 0.5118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=3, missing=None, n_estimators=24, n_jobs=1,\n",
       "              nthread=None, objective='multi:softprob', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=0,\n",
       "              silent=None, subsample=0.7, verbosity=1)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "model = xgb.XGBClassifier(learning_rate=0.1, n_estimators=24, max_depth=4, min_child_weight=3, seed=0,\n",
    "                         subsample=0.7, colsample_bytree=1, gamma=0, reg_alpha=0, reg_lambda=1)\n",
    "model.fit(train, df_target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['Parameter1', 'Parameter2', 'Parameter3', 'Parameter4', 'Parameter5', 'Parameter6', 'Parameter7', 'Parameter8', 'Parameter9', 'Parameter10'] ['Parameter1', 'Parameter2', 'Parameter3', 'Parameter4', 'Parameter5', 'Parameter6', 'Parameter7', 'Parameter8', 'Parameter9']\nexpected Parameter10 in input data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-5a5594b95c16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 测试集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtestRes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtestpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, output_margin, ntree_limit, validate_features)\u001b[0m\n\u001b[0;32m    789\u001b[0m                                                  \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m                                                  \u001b[0mntree_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mntree_limit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m                                                  validate_features=validate_features)\n\u001b[0m\u001b[0;32m    792\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[1;31m# If output_margin is active, simply return the scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1284\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m         \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[1;32m-> 1690\u001b[1;33m                                             data.feature_names))\n\u001b[0m\u001b[0;32m   1691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_split_value_histogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: feature_names mismatch: ['Parameter1', 'Parameter2', 'Parameter3', 'Parameter4', 'Parameter5', 'Parameter6', 'Parameter7', 'Parameter8', 'Parameter9', 'Parameter10'] ['Parameter1', 'Parameter2', 'Parameter3', 'Parameter4', 'Parameter5', 'Parameter6', 'Parameter7', 'Parameter8', 'Parameter9']\nexpected Parameter10 in input data"
     ]
    }
   ],
   "source": [
    "# 测试集\n",
    "testRes = model.predict(df_test.iloc[:,1:-1])\n",
    "\n",
    "testpredict = model.predict_proba(df_test.iloc[:,1:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(\n",
    "    testpredict, columns=[\"Excellent ratio\", \"Good ratio\", \"Pass ratio\", \"Fail ratio\"]\n",
    ")\n",
    "y_pred[\"Group\"] = df_test[\"Group\"]\n",
    "\n",
    "group_pred = y_pred.groupby(\"Group\").mean()\n",
    "for c in [\"Excellent ratio\", \"Good ratio\", \"Pass ratio\", \"Fail ratio\"]:\n",
    "    group_pred[c] /= group_pred.sum(axis=1)\n",
    "    \n",
    "\n",
    "group_pred.to_csv('./result1.csv', index=True, header=True)\n",
    "group_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类别结果加到原始的测试集\n",
    "df_test = pd.concat([df_test, pd.DataFrame(data=testRes)], axis=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_csv = pd.read_csv(\"submit_example.csv\")\n",
    "submit_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分组计算 概率\n",
    "g = df_test.groupby(\"Group\")\n",
    "group_list = df_test.Group.unique()\n",
    "\n",
    "for i in group_list:\n",
    "    tmp = g.get_group(i)\n",
    "    Excellent = len(tmp[tmp[0]==3])/50.0\n",
    "    Good = len(tmp[tmp[0]==2])/50.0\n",
    "    Pass = len(tmp[tmp[0]==1])/50.0\n",
    "    Fail = len(tmp[tmp[0]==0])/50.0\n",
    "    submit_csv[submit_csv[\"Group\"]==i] = [i, Excellent, Good, Pass, Fail]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submit_csv.to_csv('./result.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
